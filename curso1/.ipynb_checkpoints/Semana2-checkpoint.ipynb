{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 2: Neural Network Basics\n",
    "\n",
    "## Logistic Regression as NN\n",
    "\n",
    "### NN programing\n",
    "There's forward propagation and backward propagation\n",
    "\n",
    "* Logistic Regression: Binary classification algorithm\n",
    "\n",
    "Cat exemple: \n",
    "            0 = no cat\n",
    "            1 = cat   \n",
    "So y = 0 or 1\n",
    "\n",
    "Input = x[n]\n",
    "(vector with RGB color density of each px in the picture) <br/>\n",
    "**n** = 3*#px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainig exemple\n",
    "Notation $(x, y)$, x in $IR^n$\n",
    "\n",
    "A training set is made of **m** trainig exemples. <br/> ${(x_1, y_1),(x_2,y_2),...,(x_m,y_m)}$\n",
    "\n",
    "$$X_(n*m) =  \\begin{equation}\n",
    "  \\begin{bmatrix}\n",
    "    | & | & |\\\\\n",
    "    x_1 & x_2 & x_m\\\\\n",
    "    | & | & |\n",
    "  \\end{bmatrix}\n",
    "  \\label{eq:aeqn}\n",
    "\\end{equation}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y_(1*m)$ =  $\\begin{equation}\n",
    "  \\begin{bmatrix}\n",
    "    y_1 & y_2 & y_m  \n",
    "  \\end{bmatrix}\n",
    "  \\label{eq:aeqn}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    " Given $x$ find $\\hat{y} = $ probability of $(y=1)$\n",
    " \n",
    " $0 \\le \\hat{y} \\le 1$\n",
    " \n",
    " Output parameters:<br/>\n",
    " $w$ in $IR, b$ in $IR$\n",
    " \n",
    " $$\\hat{y} = \\sigma(w^T*x+b)$$ \n",
    " \n",
    " $$\\sigma(z) = 1/(1+e^(-z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "To train parameters w and b. It measures how well the NN is doing on the training set.\n",
    "\n",
    "$$J(w, b) = \\frac{1}{m}\\sum_{i=0}^m *L(\\hat{y_i}, y_i)$$\n",
    "\n",
    "Loss function measures how well the NN did on a particular training exemple.\n",
    "\n",
    "$$L(\\hat{y}, y) = - (y*log(\\hat{y})+(1-y)*log(1-\\hat{y})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gadient descent \n",
    "Main objective is to find $w$ and $b$ that minimize $J(w, b)$. <br/>\n",
    "It's a convex function (at least should be) with only one minimal. $w$ and $b$ can be initialized wtih any value and will always end up at the global minimun.   \n",
    "![image.png](https://miro.medium.com/max/1400/1*j4-2rXBSDEYPTQgefHX5qw.png)\n",
    "![image.png](https://hackernoon.com/hn-images/0*rBQI7uBhBKE8KT-X.png)\n",
    "\n",
    "\n",
    "\n",
    "Repeat <br/>\n",
    "$$w:=w-\\alpha*\\frac{dJ(w)}{dw}$$\n",
    " \n",
    "or $$w:=w-\\alpha*dw$$\n",
    "\n",
    "$\\alpha$ is the learning rate. It controls the step after each reetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph\n",
    "Explains why forward / backward propagation\n",
    "\n",
    "$$J(a, b, c) = 3(a+b*c)$$\n",
    "\n",
    "$u = b*c$<br/>\n",
    "$v = a+u$<br/>\n",
    "$J = 3*v$<br/>\n",
    "\n",
    "$\\frac{dJ}{dv} = 3$\n",
    "\n",
    "$\\frac{dJ}{da} = 3 = \\frac{dJ}{dv}*\\frac{dv}{da} = da$\n",
    "\n",
    "$\\frac{dv}{da} = 1$\n",
    "\n",
    "$\\frac{dJ}{du} = \\frac{dJ}{dv}*\\frac{dv}{du} = 3 $\n",
    "\n",
    "$\\frac{dJ}{db} = \\frac{dJ}{du}*\\frac{du}{db} = 6$\n",
    "\n",
    "$\\frac{dJ}{dc} = \\frac{dJ}{du}*\\frac{du}{dc} = 9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression gradient descent \n",
    "\n",
    " $$\\hat{y} = \\sigma(w^T*x+b)$$ \n",
    " $$L(\\hat{y}, y) = - (y*log(\\hat{y})+(1-y)*log(1-\\hat{y})$$\n",
    "\n",
    "$$z = w_1*x_1+w_2*x_2 + b  -->  \\hat{y} = a = \\sigma(y)  -->  $L(a, y)$$\n",
    "\n",
    "$da = \\frac{-y}{a} + \\frac{1-y}{1-a}$\n",
    "\n",
    "$dz = \\frac{dL}{da}*\\frac{da}{dz} = a-y$\n",
    "\n",
    "$dw_1 = x_1*dz$\n",
    "\n",
    "$dw_2 = x_2*dz$\n",
    "\n",
    "$db = dz$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grdient descent on m exemples\n",
    "$$J(w, b) = \\frac{1}{m}\\sum_{i=0}^m *L(\\hat{y_i}, y_i)$$\n",
    "\n",
    "$$\\frac{dJ}{dw_1} = \\frac{1}{m}*\\sum_{i=0}^m *\\frac{L(a_i, y_i}{dw_1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impport numpy as np \n",
    "J = 0\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "\n",
    "for i in range(m):\n",
    "    z[i] = w.T*x[i]+b\n",
    "    a[i] = 1(1+np.exp(-z[i]))\n",
    "    J += -(y[i]*np.log(a[i]+(1-y[i])*np.log(1-a[i])))\n",
    "    dz[i] = a[i] - y[i]\n",
    "    \n",
    "    dw1 += x1[i]*dz[i]\n",
    "    dw2 += x2[i]*dz[i]\n",
    "    db += dz[i]\n",
    "    \n",
    "J /= m\n",
    "dw1 /= m\n",
    "dw2 |= m \n",
    "db /= m\n",
    "\n",
    "w1 := w1 - alpha*dw1 \n",
    "w2 := w2 - alpha*dw2 \n",
    "b := b - alpha*db "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUUUUT should never use for loops because they reduce the efficiency. Too slow for big data sets. Use vectorization instead. But they are necessary for iterating through the exemples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and Vectorization\n",
    "### Vectorization\n",
    "$z = w^T*x+b$ \n",
    "\n",
    "$w$ and $b$ in $IR^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impport numpy as np \n",
    "J = 0\n",
    "dw = np.zeros(n, 1)\n",
    "\n",
    "for i in range(m):\n",
    "    z[i] = w.T*x[i]+b\n",
    "    a[i] = 1(1+np.exp(-z[i]))\n",
    "    J += -(y[i]*np.log(a[i]+(1-y[i])*np.log(1-a[i])))\n",
    "    dz[i] = a[i] - y[i]\n",
    "    \n",
    "    dw += x[i]*dz[i]\n",
    "    db += dz[i]\n",
    "    \n",
    "J /= m\n",
    "dw /= m\n",
    "db /= m\n",
    "\n",
    "w1 := w1 - alpha*dw1 \n",
    "w2 := w2 - alpha*dw2 \n",
    "b := b - alpha*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Logistic Regression\n",
    "$Z = [z_1, z_2, ..., z_m] = w^T*X+B$\n",
    "\n",
    "$B = [b, b, ..., b]$\n",
    "\n",
    "$A = [a_1, a_2, ..., a_m]$\n",
    "\n",
    "Z = np.dot(w.T, X)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing logistic regression's gradient output\n",
    "$dZ = [dz_1, dz_2, ..., dz_m] = A -Y$\n",
    "\n",
    "$Y = [y_1, y_2, ..., y_m]$ (y between 1 and 0) \n",
    "\n",
    "$db = \\frac{1}{m}\\sum_{i=0}^m dz_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 1/m * np.sum(dZ)\n",
    "dw = 1/m * X*dZ.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dw$ in $IR$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(w.T, X)+b\n",
    "A = sigmoid(Z)\n",
    "dZ = A - Y\n",
    "dw = 1/m * X*dZ.T\n",
    "db = 1/m * np.sum(dZ)\n",
    "\n",
    "w := w - alpha*dw\n",
    "b := b - alpah*db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
