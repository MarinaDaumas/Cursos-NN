{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 4: Special applications: Face recognition & Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face recognition\n",
    "\n",
    "Very commun use of CNN, usually associated with liveness detection to spot fake faces (ex: photographs). Usually you only have one image of each face, in a database like employees database, so you need to perform **one shot learning**.\n",
    "\n",
    "\n",
    "### One Shot Learning\n",
    "\n",
    "The standard aproach with CNN would be feed it with the images of the employees and train it to given face is output who's face is it, employeee1, 2, 3, ... or not an employee. But that would hardly work on only one image of each person and would need to train the NN every time a new person is added. \n",
    "Another option is to train a CNN to lean a **similarity function**. This way the CNN will learn to compare the images of the database and the input images to tell how different they are.\n",
    "\n",
    "$d(img1, img2) =$ degree of difference betwwen images\n",
    "\n",
    "If $d(img1, img2) <= \\sigma$ is the same person\n",
    "\n",
    "$d(img1, img2) > \\sigma$ not the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "\n",
    "This NN takes 2 images $x^{(1)}$ and $x^{(2)}$ and runs each of them separately outputing the encodes $f(x^{(1)})$ and $f(x^{(2)})$. It's made of convolutional layers and fully connected layers.\n",
    "\n",
    "\n",
    "Then it calculates \n",
    "\n",
    "$d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||^2$ \n",
    "\n",
    "and learn parameters so $d$ is close to 1 or 0 depending if it is or not the same person. One way to learn the parameters is to apply gradient descent one the Triplet Loss function. Another way is to do it as a binary classification problem.\n",
    "\n",
    "#### Triplet Loss function\n",
    "\n",
    "When running siamese networks the input will always be an anchor image $(A)$ and a compearing image wich can be a positive image $(P)$ or negatve image $(N)$. So given 3 images (a triplet) it calculates $L(A, P, N)$\n",
    "\n",
    "We want $d(A, P) <= d(A, N)$\n",
    "\n",
    "To prevent the NN from zeroing all encodings or outputing the same encode for all images, we add a margin $\\alpha$. By doing this $d(A, (P)$ and $d(A, N)$ go further away from each other.\n",
    "\n",
    "$d(A, P) - d(A, N) + \\alpha <= 0$\n",
    "\n",
    "$$L = max(d(A, P) - d(A, N) + \\alpha,  0)$$\n",
    "\n",
    "$$J = \\sum_{i = 1}^{m} L(A^{(i)}, P^{(i)}, N^{(i)})$$\n",
    "\n",
    "It's important to chose triplets in a way they are not to easy. Usually companies train these NN on with datasets of milions of exemples, many of these put their parameters online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
