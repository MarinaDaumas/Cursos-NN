{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "    \n",
    "    Try random samples. Can use coarse to fine. After fiding a region where parameters work better zoom in to that region and try random samples more densily in that.\n",
    "    \n",
    "    Some parameters should be sampled in a logarithm scale. For exemple when the value of the hypeparam is between 0.0001 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = (-4)*np.random.rand()\n",
    "alpha = 10**r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In practice there are two main ways of testing huperparameters.\n",
    "    \n",
    "- Babysitting one model day by day or few days to few days.\n",
    "- Paralel trainig. Many models at the same time, then picking up the best lkjhgtyjbv. Requires much more computational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch norm\n",
    "\n",
    "        We've seen before how normalizing the inputs helps with the shape of the cost function and finding the minimum. Now with deep NNs we can normalize the inputs from previous layers.\n",
    "        Usually the nomalization is applied to Z^[l] instead of a^[l].\n",
    "        \n",
    "- find the mean\n",
    "\n",
    "$\\mu = \\frac{1}{m}*\\sum_{i=1}^mz{(i)}$\n",
    "- find the norm\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{m}*\\sum_{i=1}^m{(z^{(i)} - \\mu)^2}$\n",
    "\n",
    "$$z^{i}_{norm}  = \\frac{z^{i}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$\n",
    "\n",
    "    Buuuuuut having mean 0 and variance 1 is not always good so we do:\n",
    "    \n",
    "$$\\hat{z}^{(i)} = \\gamma*z^{i}_{norm} + \\beta$$\n",
    "\n",
    "$\\gamma$ and $\\beta$ are learneable parameters and are updated like the NN's weights. When using bacth norm the param b^[l] ends up beeing canceled so we don't even need to use it\n",
    "    \n",
    "The parameters we should update now are : $W^{[l]}$, $\\gamma{[l]}$, $\\beta^{[l]}$. They may be updated using gradient descent or adding mommentum, RSMprop or Adam\n",
    "\n",
    "Normalizing mekes the input from previous layers shift arround much less, so it makes easier for deeper layers to adapt to changes. This makes earning faster. Also adds a little bit of noise (kind of like drop out, but much less)\n",
    "\n",
    "When testing there isn't a mini-batch to take the mean from, so we do an exponentialy weighted mean of $\\mu$ and $\\sigma^2$ during the trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "### softmax regression\n",
    "\n",
    "    When clssifying more the one class, the number of nodes in the last layer before the output should be iqual to the number of clases, including the no class. Each of the nodes will give the probability of one class.\n",
    "    The activation function of his layer should be a softmax activation, wich is a generalization of logistic regression.\n",
    "\n",
    "$l = $ number of last , $c = $ number of nodes in this layer\n",
    "\n",
    "$$a^{[l]} = \\frac{e^{z^{[l]}}}{\\sum_i^c{e_i^{z^{[l]}}}}$$\n",
    "\n",
    "The loss function then $L = -\\sum_{j=1}^{c}{y_j * \\log{ \\hat{y}_j}}$\n",
    "\n",
    "And cost is $J = \\frac{1}{m}*\\sum_{i=1}^m{L(y, \\hat{y}})$\n",
    "\n",
    "$$dz^{[l]} =  \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
