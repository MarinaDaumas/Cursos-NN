{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 2: Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch vs mini-batch gradient descent\n",
    "\n",
    "    Untill now we did batch gradient descent. We processed all our trainnig set each iteration. Mini-batching means we split our very big trainnig set into many smaller sets. \n",
    "X<sup>{t}</sup> is the t<sup>th</sup> input batch and Y<sup>{t}</sup> is the correspondig expected output.\n",
    "Their shapes should be (lenth of 1 X, number of exemples in each mini-batch) and (lenth of y, number of exemples) \n",
    "\n",
    "Its faster than batch gradient descent because the wheight are updeted many times each iterations instead of only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-11aee2bacca7>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-11aee2bacca7>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range (num_of_minibatches):\n",
    "    Z1 = W1 * X[1] + B1\n",
    "    A1 = g1(Z1) \n",
    "    Z2 = W2 * A1 + B2\n",
    "    A2 = g2(Z2)\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    Zl = Wl * Al-1 + Bl\n",
    "    Al = gl(Zl)\n",
    "    \n",
    "    # Compute cost (in this case without regularization)\n",
    "    J = 1/num_exe_in_minibatch * np.sum(L(yhat, y)) \n",
    "    \n",
    "    Back_prop(J, X[i], Y[i])\n",
    "    \n",
    "    Wl = Wl - dWl \n",
    "    Bl = Bl - dBl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    When running batch gradient descent the cost J shuold ALWAYS go down after each iterations. If it doesn't, then something is wrong. But when running mini batch, the cost will not always go dawn after an epox. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choosing mini-batch size\n",
    "\n",
    "    Too small mini-batch, for exemple stochastic gradient descent, when each trainnig exemples is it's own mini-batch, will on average go to the right direction, but can be extremelly noisy. So it never really hits the minimun it roams arround it. It also throws away any speed up you get from vectorization;\n",
    "    \n",
    "    Too big will go to the right direction with relativelly big steps and little noise. But it takes too long beetwen each iteration for too big trainning sets.\n",
    "    \n",
    "- For small trainnig sets (m<2000): Use batch gradient descent\n",
    "\n",
    "- Typical mini-batch sizes are 2 to the power of something: 64, 128, 256, 1024\n",
    "\n",
    "- Make sure it fits CPU/GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expontialy weighted moving averages\n",
    "\n",
    "$$V[t] = \\beta *V[t-1] + (1-\\beta)*\\theta[t]$$\n",
    "\n",
    "By doing this you aply a weight to previous values. If $\\beta = 0.9$, $V[t]$ deppends on the last 10 measures.\n",
    "Generalizing, d beeing the number of measures that influence V:\n",
    "\n",
    "$$d = \\frac{1}{1-\\beta}$$\n",
    "\n",
    "Ex:\n",
    "\n",
    "$v0 = 0$\n",
    "$v1 = 0.9*v0 + 0.1*\\theta1$\n",
    "$v1 = 0.9*v1 + 0.1*\\theta2$\n",
    "$v1 = 0.9*v2 + 0.1*\\theta3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias correction\n",
    "\n",
    "    As it beggins with zeros the first few averages will be much lower than they should be.\n",
    "    \n",
    "$$\\frac{V[t]}{1-\\beta^t} = \\beta*V[t-1] + (1-\\beta)*\\theta[t]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent with momentum\n",
    "    \n",
    "    We now aply moving average to the gradient descent. \n",
    "$$VdW = \\beta*VdW + (1-\\beta)*dW$$\n",
    "$$VdB = \\beta*VdB + (1-\\beta)*dB$$\n",
    "\n",
    "$W = W - \\alpha*VdW$,\n",
    "$B = B - \\alpha*VdB$\n",
    "\n",
    "    This helps because it reduces the oscilations on the gradient descent. The sum of the previous gradients will almost cancel the oscilations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "    \n",
    "    It slows down the learning in some directions\n",
    "    \n",
    "$$SdW = \\beta*SdW + (1-\\beta)*dW^2$$\n",
    "$$SdB = \\beta*SdB + (1-\\beta)*dB^2$$\n",
    "\n",
    "$W = W - \\alpha* \\frac{dW}{\\sqrt{SdW}}$,\n",
    "$B = B - \\alpha* \\frac{dB}{\\sqrt{SdB}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "    Puts gradient descent with momentum and RMSprop together.\n",
    "    t is the number of iterations on mini-batches.\n",
    "    \n",
    "$VdW_{cor} = \\frac{VdW}{(1-\\beta_v^t)}$,\n",
    "$Vdb_{cor} = \\frac{Vdb}{(1-\\beta_v^t)}$\n",
    "\n",
    "$SdW_{cor} = \\frac{SdW}{(1-\\beta_s^t)}$,\n",
    "$Sdb_{cor} = \\frac{Sdb}{(1-\\beta_s^t)}$\n",
    "\n",
    "$$W := W - \\alpha * \\frac{VdW_{cor}}{\\sqrt{SdW_{cor}} + \\epsilon}$$\n",
    "\n",
    "$$b := b - \\alpha * \\frac{Vdb_{cor}}{\\sqrt{Sdb_{cor}} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay\n",
    "\n",
    "    Usefull when you have small mini-batches. Cost moves towards the minimum but doesn't stop there. \n",
    "    In this case slowing down the learning is good to move closer to the minimun.\n",
    "    Down side is that now there's another hyperparameter to tune.\n",
    "    \n",
    "- Method 1:\n",
    "$$\\alpha = \\frac{1}{1 + decayrate*epoch} * \\alpha_0$$\n",
    "\n",
    "\n",
    "\n",
    "- Method 2(exponentialy decay):\n",
    "$$\\alpha = 0.95^epoch * \\alpha_0$$\n",
    "\n",
    "\n",
    "\n",
    "- Method 3:\n",
    "$$\\alpha = \\frac{k}{\\sqrt{epoch}} * \\alpha_0$$\n",
    "\n",
    "- Method 4:\n",
    "$$\\alpha = \\frac{k}{\\sqrt{t}} * \\alpha_0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
